import sys

try:
    import cloudscraper

    USE_CLOUDSCRAPER = True
    print("‚úì Cloudscraper ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng")
except ImportError:
    import requests

    USE_CLOUDSCRAPER = False
    print("‚ö† Cloudscraper ch∆∞a c√†i ƒë·∫∑t. ƒêang s·ª≠ d·ª•ng requests th√¥ng th∆∞·ªùng.")
    print("Ch·∫°y: pip install cloudscraper ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng bypass Cloudflare")

from bs4 import BeautifulSoup
import pandas as pd
import time
import re


def parse_value(value_str):
    """Chuy·ªÉn ƒë·ªïi chu·ªói ‚Ç¨110.5M ho·∫∑c ‚Ç¨500K th√†nh s·ªë th·ª±c"""
    if "M" in value_str:
        return float(re.sub(r"[‚Ç¨M]", "", value_str)) * 1000000
    elif "K" in value_str:
        return float(re.sub(r"[‚Ç¨K]", "", value_str)) * 1000
    return 0


def crawl_player_details(scraper, player_url):
    """Crawl th√¥ng tin chi ti·∫øt c·ªßa m·ªôt c·∫ßu th·ªß t·ª´ trang c√° nh√¢n"""
    try:
        full_url = f"https://sofifa.com{player_url}",
        response = scraper.get(full_url, timeout=15)

        if response.status_code != 200:
            print(f"  Kh√¥ng th·ªÉ truy c·∫≠p trang c·∫ßu th·ªß: {response.status_code}")
            return {}

        soup = BeautifulSoup(response.content, "html.parser")
        details = {}

        # Ph∆∞∆°ng ph√°p 1: L·∫•y t·ª´ JSON-LD (structured data)
        json_ld = soup.find("script", {"type": "application/ld+json"})
        if json_ld:
            import json

            try:
                data = json.loads(json_ld.string)
                if "height" in data:
                    height_match = re.search(r"(\d+)", data["height"])
                    if height_match:
                        details["Height_cm"] = int(height_match.group(1))
                if "weight" in data:
                    weight_match = re.search(r"(\d+)", data["weight"])
                    if weight_match:
                        details["Weight_kg"] = int(weight_match.group(1))
                if "nationality" in data:
                    details["Nationality"] = data["nationality"]
            except:
                pass

        # Ph∆∞∆°ng ph√°p 2: Parse th√¥ng tin t·ª´ profile section
        # T√¨m Preferred Foot
        page_text = soup.get_text()
        preferred_foot_match = re.search(r"Preferred foot\s+(Left|Right)", page_text)
        if preferred_foot_match:
            details["Preferred_Foot"] = preferred_foot_match.group(1)

        # ƒê·∫øm stars cho Skill Moves v√† Weak Foot
        profile_section = soup.find("div", class_="attribute")
        if profile_section:
            paragraphs = profile_section.find_all("p")
            for p in paragraphs:
                p_text = p.get_text()
                if "Skill moves" in p_text:
                    stars = p.find_all("svg", class_="star")
                    details["Skill_Moves"] = len(stars)
                elif "Weak foot" in p_text:
                    stars = p.find_all("svg", class_="star")
                    details["Weak_Foot"] = len(stars)

        # Ph∆∞∆°ng ph√°p 3: L·∫•y chi ti·∫øt stats t·ª´ c√°c th·∫ª <em>
        # T√¨m t·∫•t c·∫£ stats c√≥ d·∫°ng: <em title="value">value</em>
        stat_names = [
            "Crossing",
            "Finishing",
            "Heading accuracy",
            "Short passing",
            "Volleys",
            "Dribbling",
            "Curve",
            "FK Accuracy",
            "Long passing",
            "Ball control",
            "Acceleration",
            "Sprint speed",
            "Agility",
            "Reactions",
            "Balance",
            "Shot power",
            "Jumping",
            "Stamina",
            "Strength",
            "Long shots",
            "Aggression",
            "Interceptions",
            "Positioning",
            "Vision",
            "Penalties",
            "Composure",
            "Marking",
            "Standing tackle",
            "Sliding tackle",
        ]

        # T√¨m stats t·ª´ c√°c c·ªôt
        cols = soup.find_all("div", class_="col")
        for col in cols:
            paragraphs = col.find_all("p")
            for p in paragraphs:
                # T√¨m th·∫ª em ch·ª©a gi√° tr·ªã
                em = p.find("em")
                # T√¨m span ho·∫∑c text ch·ª©a t√™n stat
                spans = p.find_all("span")
                for span in spans:
                    span_text = span.get_text().strip()
                    if span_text in stat_names and em:
                        stat_value = em.get("title") or em.get_text()
                        if stat_value.isdigit():
                            safe_name = span_text.replace(" ", "_").replace(".", "")
                            details[safe_name] = int(stat_value)

        # L·∫•y Positions t·ª´ c√°c th·∫ª position
        positions = []
        pos_divs = soup.find_all("div", class_="pos")
        for pos_div in pos_divs[:10]:  # L·∫•y t·ªëi ƒëa 10 positions ƒë·∫ßu ti√™n
            pos_text = pos_div.get_text().strip()
            if pos_text and len(pos_text) <= 4 and pos_text not in positions:
                positions.append(pos_text)
        if positions:
            details["Positions"] = ", ".join(positions[:5])

        # Work Rate (Attack/Defense)
        work_rate_match = re.search(
            r"Work Rate.*?(\w+)\s*/\s*(\w+)", page_text, re.IGNORECASE
        )
        if work_rate_match:
            details["Work_Rate"] = (
                f"{work_rate_match.group(1)}/{work_rate_match.group(2)}"
            )

        return details

    except Exception as e:
        print(f"  L·ªói khi crawl chi ti·∫øt c·∫ßu th·ªß: {e}")
        import traceback

        traceback.print_exc()
        return {}


def load_existing_data(csv_path="sofifa_players.csv"):
    """Load d·ªØ li·ªáu ƒë√£ crawl tr∆∞·ªõc ƒë√≥ ƒë·ªÉ ti·∫øp t·ª•c"""
    try:
        if pd.io.common.file_exists(csv_path):
            df = pd.read_csv(csv_path)
            print(f"‚úì ƒê√£ load {len(df)} c·∫ßu th·ªß t·ª´ file {csv_path}")
            return df
    except Exception as e:
        print(f"‚ö† Kh√¥ng th·ªÉ load file {csv_path}: {e}")
    return pd.DataFrame()


def get_existing_player_urls(df):
    """L·∫•y danh s√°ch URL c·∫ßu th·ªß ƒë√£ crawl ƒë·ªÉ tr√°nh duplicate"""
    if df.empty or "Player_URL" not in df.columns:
        return set()
    return set(df["Player_URL"].dropna().tolist())


def crawl_sofifa(pages=5, detailed=False, start_page=0, resume=False, csv_path="sofifa_players.csv", save_interval=10):
    """
    Crawl d·ªØ li·ªáu c·∫ßu th·ªß t·ª´ SoFIFA

    Args:
        pages: S·ªë trang c·∫ßn crawl (m·ªói trang c√≥ 60 c·∫ßu th·ªß)
        detailed: N·∫øu True, s·∫Ω crawl th√¥ng tin chi ti·∫øt t·ª´ trang c√° nh√¢n c·ªßa m·ªói c·∫ßu th·ªß (ch·∫≠m h∆°n)
        start_page: Trang b·∫Øt ƒë·∫ßu crawl (0-indexed)
        resume: N·∫øu True, s·∫Ω load d·ªØ li·ªáu c≈© v√† b·ªè qua c·∫ßu th·ªß ƒë√£ c√≥
        csv_path: ƒê∆∞·ªùng d·∫´n file CSV ƒë·ªÉ l∆∞u/load
        save_interval: S·ªë trang sau m·ªói l·∫ßn t·ª± ƒë·ªông l∆∞u (ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu)
    """
    all_players = []
    existing_urls = set()
    
    # Load d·ªØ li·ªáu c≈© n·∫øu resume
    if resume:
        existing_df = load_existing_data(csv_path)
        if not existing_df.empty:
            all_players = existing_df.to_dict("records")
            existing_urls = get_existing_player_urls(existing_df)
            print(f"üìå ƒê√£ c√≥ {len(existing_urls)} URL c·∫ßu th·ªß - s·∫Ω b·ªè qua n·∫øu g·∫∑p l·∫°i")

    # T·∫°o scraper session
    if USE_CLOUDSCRAPER:
        try:
            scraper = cloudscraper.create_scraper(
                browser={"browser": "chrome", "platform": "windows", "desktop": True},
                delay=10,
            )
            print("‚úì ƒêang s·ª≠ d·ª•ng cloudscraper")
        except Exception as e:
            print(f"L·ªói cloudscraper: {e}, chuy·ªÉn sang requests")
            scraper = requests.Session()
            scraper.headers.update(
                {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.5",
                    "Referer": "https://sofifa.com/",
                }
            )
    else:
        scraper = requests.Session()
        scraper.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Referer": "https://sofifa.com/",
            }
        )

    new_players_count = 0
    skipped_count = 0
    
    for page in range(start_page, start_page + pages):
        # M·ªói trang c·ªßa SoFIFA hi·ªÉn th·ªã 60 c·∫ßu th·ªß, offset tƒÉng d·∫ßn 60
        offset = page * 60
        url = f"https://sofifa.com/players?offset={offset}"

        print(f"ƒêang crawl trang {page + 1} (offset={offset})...")

        try:
            response = scraper.get(url, timeout=15)

            if response.status_code != 200:
                print(
                    f"Kh√¥ng th·ªÉ truy c·∫≠p website - Status code: {response.status_code}"
                )
                print(f"URL: {url}")
                break
        except Exception as e:
            print(f"L·ªói k·∫øt n·ªëi: {e}")
            break

        soup = BeautifulSoup(response.content, "html.parser")
        # T√¨m b·∫£ng - website ƒë√£ thay ƒë·ªïi, kh√¥ng c√≤n class 'table-hover'
        table = soup.find("table")

        if not table:
            print("Kh√¥ng t√¨m th·∫•y b·∫£ng d·ªØ li·ªáu tr√™n trang")
            print("C√≥ th·ªÉ website ƒë√£ thay ƒë·ªïi c·∫•u tr√∫c ho·∫∑c y√™u c·∫ßu x√°c th·ª±c")
            break

        tbody = table.find("tbody")
        if not tbody:
            print("Kh√¥ng t√¨m th·∫•y tbody trong b·∫£ng")
            break

        rows = tbody.find_all("tr")

        if not rows:
            print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu c·∫ßu th·ªß tr√™n trang")
            break

        for row in rows:
            try:
                cols = row.find_all("td")

                if len(cols) < 8:
                    continue

                # Tr√≠ch xu·∫•t d·ªØ li·ªáu - c·∫•u tr√∫c c·ªôt:
                # 0: Picture, 1: Name+Position, 2: Age, 3: Overall+Change, 4: Potential+Change
                # 5: Team, 6: Value, 7: Wage, 8+: Stats

                # L·∫•y t√™n c·∫ßu th·ªß v√† URL t·ª´ c·ªôt 1
                name_col = cols[1]
                name_link = name_col.find("a")
                if not name_link:
                    continue

                # T√™n c√≥ th·ªÉ n·∫±m trong nhi·ªÅu th·∫ª, l·∫•y text v√† l√†m s·∫°ch
                name = (
                    name_link.get("title")
                    or name_link.text.strip().split("\n")[0].strip()
                )
                player_url = name_link.get("href", "")  # URL ƒë·∫øn trang chi ti·∫øt c·∫ßu th·ªß

                # B·ªè qua n·∫øu ƒë√£ c√≥ trong dataset
                if resume and player_url in existing_urls:
                    skipped_count += 1
                    continue

                age = cols[2].text.strip()
                ovr_text = cols[3].text.strip()  # C√≥ th·ªÉ l√† "75+2", "75-2" ho·∫∑c "75"
                # T√°ch s·ªë overall, b·ªè qua +/- thay ƒë·ªïi
                ovr = re.split(r"[+\-]", ovr_text)[0]

                pot_text = cols[4].text.strip()  # C√≥ th·ªÉ l√† "88+2", "88-2" ho·∫∑c "88"
                # T√°ch s·ªë potential, b·ªè qua +/- thay ƒë·ªïi
                pot = re.split(r"[+\-]", pot_text)[0]

                team_link = cols[5].find("a")
                team = team_link.text.strip() if team_link else "Free Agent"

                value = cols[6].text.strip()
                wage = cols[7].text.strip()

                player_data = {
                    "Name": name,
                    "Age": int(age) if age.isdigit() else 0,
                    "Overall": int(ovr) if ovr.isdigit() else 0,
                    "Potential": int(pot) if pot.isdigit() else 0,
                    "Team": team,
                    "Value_Raw": value,
                    "Wage_Raw": wage,
                    "Value_Numeric": parse_value(value),
                    "Wage_Numeric": parse_value(wage),
                    "Player_URL": player_url,
                }

                # N·∫øu y√™u c·∫ßu th√¥ng tin chi ti·∫øt, crawl trang c√° nh√¢n c·ªßa c·∫ßu th·ªß
                if detailed and player_url:
                    print(f"  ‚Üí ƒêang l·∫•y th√¥ng tin chi ti·∫øt c·ªßa {name}...")
                    detailed_info = crawl_player_details(scraper, player_url)
                    player_data.update(detailed_info)
                    time.sleep(1)  # Ngh·ªâ th√™m khi crawl chi ti·∫øt ƒë·ªÉ tr√°nh b·ªã ban

                all_players.append(player_data)
                existing_urls.add(player_url)  # Th√™m v√†o set ƒë·ªÉ tr√°nh duplicate trong c√πng session
                new_players_count += 1
            except Exception as e:
                print(f"L·ªói khi x·ª≠ l√Ω m·ªôt c·∫ßu th·ªß: {e}")
                continue

        # Auto-save sau m·ªói save_interval trang
        pages_crawled = page - start_page + 1
        if save_interval > 0 and pages_crawled % save_interval == 0:
            temp_df = pd.DataFrame(all_players)
            temp_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
            print(f"üíæ T·ª± ƒë·ªông l∆∞u: {len(all_players)} c·∫ßu th·ªß (m·ªõi: {new_players_count}, b·ªè qua: {skipped_count})")

        # Ngh·ªâ m·ªôt ch√∫t ƒë·ªÉ tr√°nh b·ªã ban IP
        time.sleep(2)

    print(f"\nüìä K·∫øt qu·∫£ crawl: {new_players_count} c·∫ßu th·ªß m·ªõi, {skipped_count} b·ªè qua (ƒë√£ c√≥)")
    return pd.DataFrame(all_players)


def test_connection():
    """Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn website"""
    if USE_CLOUDSCRAPER:
        try:
            scraper = cloudscraper.create_scraper(
                browser={"browser": "chrome", "platform": "windows", "desktop": True},
                delay=10,
            )
            print("‚Üí ƒêang s·ª≠ d·ª•ng cloudscraper ƒë·ªÉ bypass Cloudflare")
        except Exception as e:
            print(f"L·ªói kh·ªüi t·∫°o cloudscraper: {e}")
            scraper = requests.Session()
            scraper.headers.update(
                {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.5",
                }
            )
    else:
        scraper = requests.Session()
        scraper.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Referer": "https://sofifa.com/",
            }
        )
        print("‚Üí ƒêang s·ª≠ d·ª•ng requests v·ªõi headers m√¥ ph·ªèng tr√¨nh duy·ªát")

    url = "https://sofifa.com/players"
    print(f"ƒêang ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn: {url}")

    try:
        response = scraper.get(url, timeout=15)
        print(f"Status code: {response.status_code}")
        print(f"Content length: {len(response.content)} bytes")

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")

            # Debug: In ra t·∫•t c·∫£ c√°c class c·ªßa b·∫£ng ƒë·ªÉ ki·ªÉm tra
            tables = soup.find_all("table")
            print(f"T√¨m th·∫•y {len(tables)} b·∫£ng")
            for i, tbl in enumerate(tables):
                print(f"  B·∫£ng {i+1}: class={tbl.get('class')}")

            table = soup.find("table", {"class": "table-hover"})
            if not table:
                # Th·ª≠ t√¨m b·∫£ng v·ªõi class kh√°c
                table = soup.find("table")

            print(f"T√¨m th·∫•y b·∫£ng: {table is not None}")
            if table:
                tbody = table.find("tbody")
                rows = tbody.find_all("tr") if tbody else []
                print(f"S·ªë d√≤ng d·ªØ li·ªáu: {len(rows)}")

                # Debug: In ra c·∫•u tr√∫c c·ªßa d√≤ng ƒë·∫ßu ti√™n
                if rows:
                    first_row = rows[0]
                    cols = first_row.find_all(["td", "th"])
                    print(f"S·ªë c·ªôt: {len(cols)}")
                    for idx, col in enumerate(cols[:5]):  # In 5 c·ªôt ƒë·∫ßu
                        print(f"  C·ªôt {idx}: {col.text.strip()[:50]}")
        return response
    except Exception as e:
        print(f"L·ªói: {e}")
        return None


# Ki·ªÉm tra k·∫øt n·ªëi tr∆∞·ªõc
print("=== KI·ªÇM TRA K·∫æT N·ªêI ===")
test_response = test_connection()
print()

if test_response and test_response.status_code == 200:
    print("=== B·∫ÆT ƒê·∫¶U CRAWL D·ªÆ LI·ªÜU ===")
    print()

    # C·∫§U H√åNH CRAWL - Thay ƒë·ªïi c√°c gi√° tr·ªã n√†y theo nhu c·∫ßu
    # ============================================================
    RESUME_MODE = True      # True = ti·∫øp t·ª•c crawl t·ª´ d·ªØ li·ªáu c≈©, False = b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu
    START_PAGE = 141         # Trang b·∫Øt ƒë·∫ßu (0-indexed). 5442 records / 60 = ~91 trang ƒë√£ crawl
    NUM_PAGES = 50          # S·ªë trang c·∫ßn crawl th√™m (m·ªói trang 60 c·∫ßu th·ªß)
    DETAILED_MODE = True    # True = l·∫•y th√¥ng tin chi ti·∫øt (ch·∫≠m h∆°n), False = ch·ªâ l·∫•y th√¥ng tin c∆° b·∫£n
    SAVE_INTERVAL = 5       # T·ª± ƒë·ªông l∆∞u sau m·ªói bao nhi√™u trang (0 = kh√¥ng auto-save)
    CSV_PATH = "sofifa_players.csv"
    # ============================================================

    print(f"üìä C·∫•u h√¨nh:")
    print(f"   - Resume mode: {'B·∫¨T' if RESUME_MODE else 'T·∫ÆT'}")
    print(f"   - Trang b·∫Øt ƒë·∫ßu: {START_PAGE + 1} (offset={START_PAGE * 60})")
    print(f"   - S·ªë trang crawl: {NUM_PAGES}")
    print(f"   - Mode: {'CHI TI·∫æT' if DETAILED_MODE else 'C∆† B·∫¢N'}")
    print(f"   - Auto-save: m·ªói {SAVE_INTERVAL} trang")
    print(f"   - ∆Ø·ªõc t√≠nh th√™m: ~{NUM_PAGES * 60} c·∫ßu th·ªß m·ªõi")
    if DETAILED_MODE:
        print("‚ö† Ch·∫ø ƒë·ªô chi ti·∫øt s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian h∆°n")
    print()

    df = crawl_sofifa(
        pages=NUM_PAGES, 
        detailed=DETAILED_MODE, 
        start_page=START_PAGE,
        resume=RESUME_MODE,
        csv_path=CSV_PATH,
        save_interval=SAVE_INTERVAL
    )
else:
    print("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn website. T·∫°o DataFrame r·ªóng.")
    df = pd.DataFrame()

# L∆∞u d·ªØ li·ªáu ra CSV
if not df.empty:
    df.to_csv("sofifa_players.csv", index=False, encoding="utf-8-sig")
    print(f"\n‚úì ƒê√£ l∆∞u {len(df)} c·∫ßu th·ªß v√†o file sofifa_players.csv")
    print(f"\nüìã C√°c c·ªôt d·ªØ li·ªáu: {', '.join(df.columns.tolist())}")
    print(f"\nüëÄ Preview 5 c·∫ßu th·ªß cu·ªëi c√πng:")
    print(df.tail())
else:
    print("\n‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
