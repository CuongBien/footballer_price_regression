import numpy as np
import pandas as pd
from typing import Union, Dict, Any

from Metrics.MSE import compute_MSE_Reduction
from Metrics.MAE import compute_MAE_Reduction


class Node:
    """ƒê·∫°i di·ªán cho m·ªôt n√∫t trong c√¢y h·ªìi quy"""
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature      # Thu·ªôc t√≠nh ƒë·ªÉ split
        self.threshold = threshold  # Gi√° tr·ªã ƒë·ªÉ split (cho continuous) ho·∫∑c None
        self.left = left           # Nh√°nh tr√°i
        self.right = right         # Nh√°nh ph·∫£i
        self.value = value         # Gi√° tr·ªã d·ª± ƒëo√°n n·∫øu l√† l√°
        self.children = {}         # Dictionary cho categorical splits
        
    def is_leaf(self):
        return self.value is not None


class RegressionTree:
    """C√¢y h·ªìi quy h·ªó tr·ª£ c·∫£ MSE v√† MAE"""
    
    def __init__(self, criterion='mse', max_depth=None, min_samples_split=2):
        self.criterion = criterion
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
        self.feature_names = None
        self.target_name = None
        
    def find_best_split(self, data, features, target):
        """T√¨m feature t·ªët nh·∫•t ƒë·ªÉ split - s·ª≠ d·ª•ng c√°c module ƒë√£ import"""
        best_reduction = -1
        best_feature = None
        
        for feature in features:
            if self.criterion == 'mse':
                reduction = compute_MSE_Reduction(data, feature, target)
            else:
                reduction = compute_MAE_Reduction(data, feature, target)
            
            if reduction > best_reduction:
                best_reduction = reduction
                best_feature = feature
                
        return best_feature, best_reduction
    
    def leaf_value(self, target_column):
        if self.criterion == 'mse':
            return np.mean(target_column)
        else:
            return np.median(target_column)
    
    def build_tree(self, data, features, target, depth=0):
        """X√¢y d·ª±ng c√¢y ƒë·ªá quy"""
        target_column = data[target]
        
        # ƒêi·ªÅu ki·ªán d·ª´ng: n·∫øu kh√¥ng c√≤n features ho·∫∑c ƒë·∫°t max_depth
        if len(features) == 0 or (self.max_depth is not None and depth >= self.max_depth):
            return Node(value=self.leaf_value(target_column))
        
        # N·∫øu √≠t h∆°n min_samples_split, tr·∫£ v·ªÅ l√°
        if len(data) < self.min_samples_split:
            return Node(value=self.leaf_value(target_column))
        
        # N·∫øu t·∫•t c·∫£ gi√° tr·ªã target gi·ªëng nhau, tr·∫£ v·ªÅ l√°
        if len(np.unique(target_column)) == 1:
            return Node(value=target_column.iloc[0])
        
        # T√¨m feature t·ªët nh·∫•t ƒë·ªÉ split
        best_feature, best_reduction = self.find_best_split(data, features, target)
        
        # N·∫øu kh√¥ng c√≥ reduction (ho·∫∑c reduction = 0), tr·∫£ v·ªÅ l√°
        if best_reduction <= 0:
            return Node(value=self.leaf_value(target_column))
        
        # T·∫°o node v·ªõi feature t·ªët nh·∫•t
        node = Node(feature=best_feature)
        
        # L·∫•y c√°c gi√° tr·ªã unique c·ªßa feature
        unique_values = data[best_feature].unique()
        
        # T·∫°o c√°c nh√°nh con cho m·ªói gi√° tr·ªã
        remaining_features = [f for f in features if f != best_feature]
        
        for value in unique_values:
            subset = data[data[best_feature] == value]
            if len(subset) == 0:
                # N·∫øu subset r·ªóng, t·∫°o l√° v·ªõi gi√° tr·ªã trung b√¨nh
                node.children[value] = Node(value=self.leaf_value(target_column))
            else:
                # ƒê·ªá quy x√¢y d·ª±ng c√¢y con
                node.children[value] = self.build_tree(subset, remaining_features, target, depth + 1)
        
        return node
    
    def fit(self, data, target_name):
        """
        Hu·∫•n luy·ªán c√¢y h·ªìi quy
        """
        self.target_name = target_name
        self.feature_names = [col for col in data.columns if col != target_name]
        self.root = self.build_tree(data, self.feature_names, target_name)
        return self
    
    def predict_single(self, node, sample):
        """D·ª± ƒëo√°n cho m·ªôt m·∫´u"""
        if node.is_leaf():
            return node.value
        
        feature_value = sample[node.feature]
        
        # N·∫øu gi√° tr·ªã n√†y t·ªìn t·∫°i trong children
        if feature_value in node.children:
            return self.predict_single(node.children[feature_value], sample)
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y gi√° tr·ªã, tr·∫£ v·ªÅ gi√° tr·ªã c·ªßa nh√°nh ƒë·∫ßu ti√™n (fallback)
            if len(node.children) > 0:
                first_child = list(node.children.values())[0]
                return self.predict_single(first_child, sample)
            return node.value if node.is_leaf() else 0
    
    def predict(self, data):
        """
        D·ª± ƒëo√°n cho nhi·ªÅu m·∫´u
        """
        predictions = []
        for idx in range(len(data)):
            sample = data.iloc[idx]
            prediction = self.predict_single(self.root, sample)
            predictions.append(prediction)
        return predictions
    
    def print_tree(self, node=None, depth=0, prefix="Root"):
        """In c·∫•u tr√∫c c√¢y"""
        if node is None:
            node = self.root
            
        indent = "  " * depth
        
        if node.is_leaf():
            print(f"{indent}{prefix} -> D·ª± ƒëo√°n: {node.value:.2f}")
        else:
            print(f"{indent}{prefix} -> Split theo: {node.feature}")
            for value, child in node.children.items():
                self.print_tree(child, depth + 1, f"{node.feature} = {value}")
    
    def score(self, data, target_name):
        """
        T√≠nh R¬≤ score (coefficient of determination)
        """
        predictions = self.predict(data)
        actual = data[target_name].values
        
        # T√≠nh R¬≤ score
        ss_res = np.sum((actual - predictions) ** 2)
        ss_tot = np.sum((actual - np.mean(actual)) ** 2)
        
        if ss_tot == 0:
            return 0
        
        r2 = 1 - (ss_res / ss_tot)
        return r2
    
    def mse_score(self, data, target_name):
        """T√≠nh Mean Squared Error"""
        predictions = self.predict(data)
        actual = data[target_name].values
        mse = np.mean((actual - predictions) ** 2)
        return mse
    
    def mae_score(self, data, target_name):
        """T√≠nh Mean Absolute Error"""
        predictions = self.predict(data)
        actual = data[target_name].values
        mae = np.mean(np.abs(actual - predictions))
        return mae


# === DEMO ===
if __name__ == "__main__":
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(current_dir, '../../data/sofifa_players.csv')
    df = pd.read_csv(csv_path)
    
    # Ch·ªçn c√°c features v√† target
    selected_cols = ['Age', 'Overall', 'Potential', 'Wage_Numeric', 'Value_Numeric']
    df = df[selected_cols].dropna()
    
    # Lo·∫°i b·ªè c√°c h√†ng c√≥ Value_Numeric = 0
    df = df[df['Value_Numeric'] > 0]
    
    # Chuy·ªÉn Value sang tri·ªáu ‚Ç¨ ƒë·ªÉ MSE d·ªÖ ƒë·ªçc h∆°n
    df['Value_Million'] = df['Value_Numeric'] / 1_000_000
    
    # Discretize numeric features
    def discretize_column(col, bins, labels):
        return pd.cut(col, bins=bins, labels=labels, include_lowest=True)
    
    df_binned = df.copy()
    df_binned['Age'] = discretize_column(df['Age'], bins=[0, 22, 28, 35, 50], labels=['Young', 'Prime', 'Experienced', 'Veteran'])
    df_binned['Overall'] = discretize_column(df['Overall'], bins=[0, 70, 80, 85, 100], labels=['Low', 'Medium', 'High', 'Elite'])
    df_binned['Potential'] = discretize_column(df['Potential'], bins=[0, 75, 85, 90, 100], labels=['Low', 'Medium', 'High', 'Elite'])
    df_binned['Wage_Numeric'] = discretize_column(df['Wage_Numeric'], bins=[-1, 30000, 80000, 150000, 1000000], labels=['Low', 'Medium', 'High', 'Elite'])
    
    # Balanced sampling: 100 samples per Overall category
    samples_per_class = 100
    balanced_dfs = []
    for category in ['Low', 'Medium', 'High', 'Elite']:
        category_df = df_binned[df_binned['Overall'] == category]
        n_samples = min(samples_per_class, len(category_df))
        if n_samples > 0:
            sampled = category_df.sample(n=n_samples, random_state=42)
            balanced_dfs.append(sampled)
            print(f"Sampled {n_samples} from Overall={category}")
    
    df_binned = pd.concat(balanced_dfs, ignore_index=True)
    
    target_column = 'Value_Million'  # D√πng tri·ªáu ‚Ç¨ thay v√¨ ‚Ç¨
    features = ['Age', 'Overall', 'Potential', 'Wage_Numeric']
    
    print("=" * 60)
    print("DEMO C√ÇY H·ªíI QUY (REGRESSION TREE)")
    print("D·ª± ƒëo√°n gi√° tr·ªã c·∫ßu th·ªß (ƒë∆°n v·ªã: tri·ªáu ‚Ç¨)")
    print("=" * 60)
    print(f"\nS·ªë l∆∞·ª£ng m·∫´u: {len(df_binned)}")
    print(f"Features: {features}")
    print(f"Target: {target_column}")
    print(f"\nTh·ªëng k√™ gi√° tr·ªã (tri·ªáu ‚Ç¨):")
    print(f"  Min: {df_binned[target_column].min():.2f}M‚Ç¨")
    print(f"  Max: {df_binned[target_column].max():.2f}M‚Ç¨")
    print(f"  Mean: {df_binned[target_column].mean():.2f}M‚Ç¨")
    print("\nD·ªØ li·ªáu m·∫´u:")
    print(df_binned[features + [target_column]].head(10).to_string())
    
    # T·∫°o dataframe cho training
    train_df = df_binned[features + [target_column]].copy()
    
    # Test v·ªõi MSE
    print("\n" + "=" * 60)
    print("1. C√ÇY H·ªíI QUY V·ªöI MSE")
    print("=" * 60)
    tree_mse = RegressionTree(criterion='mse', max_depth=3)
    tree_mse.fit(train_df, target_column)
    print("\nC·∫•u tr√∫c c√¢y:")
    tree_mse.print_tree()
    
    r2 = tree_mse.score(train_df, target_column)
    mse = tree_mse.mse_score(train_df, target_column)
    rmse = np.sqrt(mse)
    mae = tree_mse.mae_score(train_df, target_column)
    
    # T√≠nh MAPE (Mean Absolute Percentage Error)
    predictions = tree_mse.predict(train_df)
    actual = train_df[target_column].values
    mape = np.mean(np.abs((actual - predictions) / actual)) * 100
    
    print(f"\nüìä K·∫øt qu·∫£ Regression v·ªõi MSE:")
    print(f"  R¬≤ Score: {r2:.4f} (gi·∫£i th√≠ch {r2*100:.1f}% variance)")
    print(f"  RMSE: {rmse:.2f} tri·ªáu ‚Ç¨ (sai s·ªë trung b√¨nh)")
    print(f"  MAE: {mae:.2f} tri·ªáu ‚Ç¨") 
    print(f"  MAPE: {mape:.1f}% (sai s·ªë ph·∫ßn trƒÉm)")
    
    # Test v·ªõi MAE
    print("\n" + "=" * 60)
    print("2. C√ÇY H·ªíI QUY V·ªöI MAE")
    print("=" * 60)
    tree_mae = RegressionTree(criterion='mae', max_depth=3)
    tree_mae.fit(train_df, target_column)
    print("\nC·∫•u tr√∫c c√¢y:")
    tree_mae.print_tree()
    
    r2_mae = tree_mae.score(train_df, target_column)
    mse_mae = tree_mae.mse_score(train_df, target_column)
    rmse_mae = np.sqrt(mse_mae)
    mae_val = tree_mae.mae_score(train_df, target_column)
    
    predictions_mae_tree = tree_mae.predict(train_df)
    mape_mae = np.mean(np.abs((actual - predictions_mae_tree) / actual)) * 100
    
    print(f"\nüìä K·∫øt qu·∫£ Regression v·ªõi MAE:")
    print(f"  R¬≤ Score: {r2_mae:.4f} (gi·∫£i th√≠ch {r2_mae*100:.1f}% variance)")
    print(f"  RMSE: {rmse_mae:.2f} tri·ªáu ‚Ç¨")
    print(f"  MAE: {mae_val:.2f} tri·ªáu ‚Ç¨")
    print(f"  MAPE: {mape_mae:.1f}%")
    
    # Test d·ª± ƒëo√°n
    print("\n" + "=" * 60)
    print("3. D·ª∞ ƒêO√ÅN M·∫™U M·ªöI")
    print("=" * 60)
    test_data = pd.DataFrame({
        'Age': ['Young', 'Prime', 'Experienced', 'Veteran'],
        'Overall': ['Elite', 'High', 'Medium', 'Low'],
        'Potential': ['Elite', 'High', 'Medium', 'Low'],
        'Wage_Numeric': ['Elite', 'High', 'Medium', 'Low']
    })
    print("\nD·ªØ li·ªáu test (4 c·∫ßu th·ªß m·∫´u):")
    print(test_data)
    
    predictions_mse = tree_mse.predict(test_data)
    predictions_mae = tree_mae.predict(test_data)
    
    print("\nK·∫øt qu·∫£ d·ª± ƒëo√°n gi√° tr·ªã c·∫ßu th·ªß:")
    print(f"MSE criterion: {[f'{p:.1f}M‚Ç¨' for p in predictions_mse]}")
    print(f"MAE criterion: {[f'{p:.1f}M‚Ç¨' for p in predictions_mae]}")
    print("=" * 60)

