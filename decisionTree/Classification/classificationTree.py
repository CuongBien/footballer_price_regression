import numpy as np
import pandas as pd
from typing import Union, Dict, Any


def compute_entropy(target_column):
    """TÃ­nh entropy cá»§a má»™t cá»™t target"""
    elements, counts = np.unique(target_column, return_counts=True)
    entropy = -np.sum([(count/np.sum(counts)) * np.log2(count/np.sum(counts)) 
                       for count in counts])
    return entropy


def compute_information_gain(data, split_feature, target_name):
    """TÃ­nh Information Gain khi split theo feature"""
    total_entropy = compute_entropy(data[target_name])
    
    vals, counts = np.unique(data[split_feature], return_counts=True)
    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) * 
                               compute_entropy(data.where(data[split_feature]==vals[i]).dropna()[target_name])
                               for i in range(len(vals))])
    
    information_gain = total_entropy - weighted_entropy
    return information_gain


def compute_gini_impurity(target_column):
    """TÃ­nh Gini Impurity"""
    elements, counts = np.unique(target_column, return_counts=True)
    probabilities = counts / np.sum(counts)
    gini = 1 - np.sum(probabilities ** 2)
    return gini


def compute_Gini_Gain(data, split_feature, target_name):
    """TÃ­nh Gini Gain khi split theo feature"""
    total_gini = compute_gini_impurity(data[target_name])
    
    vals, counts = np.unique(data[split_feature], return_counts=True)
    weighted_gini = np.sum([(counts[i]/np.sum(counts)) * 
                            compute_gini_impurity(data.where(data[split_feature]==vals[i]).dropna()[target_name])
                            for i in range(len(vals))])
    
    gini_gain = total_gini - weighted_gini
    return gini_gain


class Node:
    """Äáº¡i diá»‡n cho má»™t nÃºt trong cÃ¢y quyáº¿t Ä‘á»‹nh"""
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature      # Thuá»™c tÃ­nh Ä‘á»ƒ split
        self.threshold = threshold  # GiÃ¡ trá»‹ Ä‘á»ƒ split (cho continuous) hoáº·c None
        self.left = left           # NhÃ¡nh trÃ¡i
        self.right = right         # NhÃ¡nh pháº£i
        self.value = value         # GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n náº¿u lÃ  lÃ¡
        self.children = {}         # Dictionary cho categorical splits
        
    def is_leaf(self):
        return self.value is not None


class DecisionTree:
    """CÃ¢y quyáº¿t Ä‘á»‹nh há»— trá»£ cáº£ Information Gain vÃ  Gini Impurity"""
    def __init__(self, criterion='information_gain', max_depth=None, min_samples_split=2):
        self.criterion = criterion
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
        self.feature_names = None
        self.target_name = None
    
    def find_best_split(self, data, features, target):
        """TÃ¬m feature tá»‘t nháº¥t Ä‘á»ƒ split"""
        best_gain = -1
        best_feature = None
        
        for feature in features:
            if self.criterion == 'information_gain':
                gain = compute_information_gain(data, feature, target)
            else:
                gain = compute_Gini_Gain(data, feature, target)
            
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
                
        return best_feature, best_gain
    
    def most_common_label(self, target_column):
        """Tráº£ vá» nhÃ£n phá»• biáº¿n nháº¥t"""
        return target_column.mode()[0]
    
    def build_tree(self, data, features, target, depth=0):
        """XÃ¢y dá»±ng cÃ¢y Ä‘á»‡ quy"""
        # Äiá»u kiá»‡n dá»«ng
        target_column = data[target]
        
        # Náº¿u táº¥t cáº£ nhÃ£n giá»‘ng nhau, tráº£ vá» lÃ¡
        if len(np.unique(target_column)) == 1:
            return Node(value=target_column.iloc[0])
        
        # Náº¿u khÃ´ng cÃ²n features hoáº·c Ä‘áº¡t max_depth, tráº£ vá» lÃ¡ vá»›i nhÃ£n phá»• biáº¿n nháº¥t
        if len(features) == 0 or (self.max_depth is not None and depth >= self.max_depth):
            return Node(value=self.most_common_label(target_column))
        
        # Náº¿u Ã­t hÆ¡n min_samples_split, tráº£ vá» lÃ¡
        if len(data) < self.min_samples_split:
            return Node(value=self.most_common_label(target_column))
        
        # TÃ¬m feature tá»‘t nháº¥t Ä‘á»ƒ split
        best_feature, best_gain = self.find_best_split(data, features, target)
        
        # Náº¿u khÃ´ng cÃ³ gain, tráº£ vá» lÃ¡
        if best_gain == 0:
            return Node(value=self.most_common_label(target_column))
        
        # Táº¡o node vá»›i feature tá»‘t nháº¥t
        node = Node(feature=best_feature)
        
        # Láº¥y cÃ¡c giÃ¡ trá»‹ unique cá»§a feature
        unique_values = data[best_feature].unique()
        
        # Táº¡o cÃ¡c nhÃ¡nh con cho má»—i giÃ¡ trá»‹
        remaining_features = [f for f in features if f != best_feature]
        
        for value in unique_values:
            subset = data[data[best_feature] == value]
            if len(subset) == 0:
                # Náº¿u subset rá»—ng, táº¡o lÃ¡ vá»›i nhÃ£n phá»• biáº¿n nháº¥t
                node.children[value] = Node(value=self.most_common_label(target_column))
            else:
                # Äá»‡ quy xÃ¢y dá»±ng cÃ¢y con
                node.children[value] = self.build_tree(subset, remaining_features, target, depth + 1)
        
        return node
    
    def fit(self, data, target_name):
        self.target_name = target_name
        self.feature_names = [col for col in data.columns if col != target_name]
        self.root = self.build_tree(data, self.feature_names, target_name)
        return self
    
    def predict_single(self, node, sample):
        """Dá»± Ä‘oÃ¡n cho má»™t máº«u"""
        if node.is_leaf():
            return node.value
        
        feature_value = sample[node.feature]
        
        # Náº¿u giÃ¡ trá»‹ nÃ y tá»“n táº¡i trong children
        if feature_value in node.children:
            return self.predict_single(node.children[feature_value], sample)
        else:
            # Náº¿u khÃ´ng tÃ¬m tháº¥y giÃ¡ trá»‹, tráº£ vá» giÃ¡ trá»‹ cá»§a nhÃ¡nh Ä‘áº§u tiÃªn (fallback)
            if len(node.children) > 0:
                first_child = list(node.children.values())[0]
                return self.predict_single(first_child, sample)
            return None
    
    def predict(self, data):
        """
        Dá»± Ä‘oÃ¡n cho nhiá»u máº«u
        """
        predictions = []
        for idx in range(len(data)):
            sample = data.iloc[idx]
            prediction = self.predict_single(self.root, sample)
            predictions.append(prediction)
        return predictions
    
    def print_tree(self, node=None, depth=0, prefix="Root"):
        """In cáº¥u trÃºc cÃ¢y"""
        if node is None:
            node = self.root
            
        indent = "  " * depth
        
        if node.is_leaf():
            print(f"{indent}{prefix} -> Dá»± Ä‘oÃ¡n: {node.value}")
        else:
            print(f"{indent}{prefix} -> Split theo: {node.feature}")
            for value, child in node.children.items():
                self.print_tree(child, depth + 1, f"{node.feature} = {value}")
    
    def score(self, data, target_name):
        """TÃ­nh accuracy"""
        predictions = self.predict(data)
        actual = data[target_name].values
        correct = sum([1 for pred, act in zip(predictions, actual) if pred == act])
        return correct / len(actual)

# === DEMO ===
if __name__ == "__main__":
    # Äá»c dá»¯ liá»‡u FIFA
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(current_dir, '../../data/sofifa_players.csv')
    df = pd.read_csv(csv_path)
    
    # Chá»n cÃ¡c features vÃ  target
    selected_cols = ['Age', 'Overall', 'Potential', 'Wage_Numeric', 'Value_Numeric']
    df = df[selected_cols].dropna()
    
    # Loáº¡i bá» cÃ¡c hÃ ng cÃ³ Value_Numeric = 0
    df = df[df['Value_Numeric'] > 0]
    
    # Discretize numeric features thÃ nh categorical bins
    def discretize_column(col, bins, labels):
        return pd.cut(col, bins=bins, labels=labels, include_lowest=True)
    
    df_binned = df.copy()
    df_binned['Age_Cat'] = discretize_column(df['Age'], bins=[0, 22, 28, 35, 50], labels=['Young', 'Prime', 'Experienced', 'Veteran'])
    df_binned['Potential_Cat'] = discretize_column(df['Potential'], bins=[0, 75, 85, 90, 100], labels=['Low', 'Medium', 'High', 'Elite'])
    df_binned['Wage_Cat'] = discretize_column(df['Wage_Numeric'], bins=[-1, 30000, 80000, 150000, 1000000], labels=['Low', 'Medium', 'High', 'Elite'])
    
    # Target: PhÃ¢n loáº¡i cáº§u thá»§ theo má»©c Ä‘Ã¡nh giÃ¡ Overall (há»£p lÃ½ hÆ¡n Preferred_Foot)
    df_binned['Overall_Category'] = discretize_column(df['Overall'], bins=[0, 70, 80, 85, 100], labels=['Low', 'Medium', 'High', 'Elite'])
    
    # Balanced sampling: 100 samples per Overall_Category
    samples_per_class = 100
    balanced_dfs = []
    for category in ['Low', 'Medium', 'High', 'Elite']:
        category_df = df_binned[df_binned['Overall_Category'] == category]
        n_samples = min(samples_per_class, len(category_df))
        if n_samples > 0:
            sampled = category_df.sample(n=n_samples, random_state=42)
            balanced_dfs.append(sampled)
            print(f"Sampled {n_samples} from Overall_Category={category}")
    
    df_binned = pd.concat(balanced_dfs, ignore_index=True)
    
    target_column = 'Overall_Category'
    features = ['Age_Cat', 'Potential_Cat', 'Wage_Cat']
    
    train_df = df_binned[features + [target_column]].copy()
    
    print("=" * 60)
    print("DEMO CÃ‚Y QUYáº¾T Äá»ŠNH (CLASSIFICATION TREE)")
    print("Dá»± Ä‘oÃ¡n má»©c Ä‘Ã¡nh giÃ¡ cáº§u thá»§ (Overall_Category)")
    print("=" * 60)
    print(f"\nSá»‘ lÆ°á»£ng máº«u: {len(train_df)}")
    print(f"Features: {features}")
    print(f"Target: {target_column}")
    print(f"\nPhÃ¢n phá»‘i target:")
    print(train_df[target_column].value_counts().sort_index())
    print(f"\nBaseline accuracy (Ä‘oÃ¡n class phá»• biáº¿n nháº¥t): {train_df[target_column].value_counts().max() / len(train_df) * 100:.1f}%")
    print("\nDá»¯ liá»‡u máº«u:")
    print(train_df.head(10).to_string())
    
    # Test vá»›i Information Gain
    print("\n" + "=" * 60)
    print("1. CÃ‚Y QUYáº¾T Äá»ŠNH Vá»šI INFORMATION GAIN")
    print("=" * 60)
    tree_ig = DecisionTree(criterion='information_gain', max_depth=4)
    tree_ig.fit(train_df, target_column)
    print("\nCáº¥u trÃºc cÃ¢y:")
    tree_ig.print_tree()
    
    accuracy_ig = tree_ig.score(train_df, target_column)
    print(f"\nğŸ“Š Káº¿t quáº£ Classification vá»›i Information Gain:")
    print(f"  Accuracy: {accuracy_ig * 100:.2f}%")
    
    # Test vá»›i Gini
    print("\n" + "=" * 60)
    print("2. CÃ‚Y QUYáº¾T Äá»ŠNH Vá»šI GINI IMPURITY")
    print("=" * 60)
    tree_gini = DecisionTree(criterion='gini', max_depth=4)
    tree_gini.fit(train_df, target_column)
    print("\nCáº¥u trÃºc cÃ¢y:")
    tree_gini.print_tree()
    
    accuracy_gini = tree_gini.score(train_df, target_column)
    print(f"\nğŸ“Š Káº¿t quáº£ Classification vá»›i Gini Impurity:")
    print(f"  Accuracy: {accuracy_gini * 100:.2f}%")
    
    # Test dá»± Ä‘oÃ¡n
    print("\n" + "=" * 60)
    print("3. Dá»° ÄOÃN MáºªU Má»šI")
    print("=" * 60)
    test_data = pd.DataFrame({
        'Age_Cat': ['Young', 'Prime', 'Experienced', 'Veteran'],
        'Potential_Cat': ['Elite', 'High', 'Medium', 'Low'],
        'Wage_Cat': ['Elite', 'High', 'Medium', 'Low']
    })
    print("\nDá»¯ liá»‡u test (4 cáº§u thá»§ máº«u):")
    print(test_data.to_string())
    
    predictions_ig = tree_ig.predict(test_data)
    predictions_gini = tree_gini.predict(test_data)
    
    print("\nKáº¿t quáº£ dá»± Ä‘oÃ¡n má»©c Overall:")
    print(f"Information Gain: {predictions_ig}")
    print(f"Gini Impurity:    {predictions_gini}")
    
    print("\n" + "=" * 60)
    print("Tá»”NG Káº¾T")
    print("=" * 60)
    print(f"Information Gain Accuracy: {accuracy_ig * 100:.2f}%")
    print(f"Gini Impurity Accuracy:    {accuracy_gini * 100:.2f}%")
    print("=" * 60)


# Alias for sklearn-like naming
DecisionTreeClassifier = DecisionTree
